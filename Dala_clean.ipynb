{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d518aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bde6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0                            i didnt feel humiliated       7\n",
       "1  i can go from feeling so hopeless to so damned...       7\n",
       "2   im grabbing a minute to post i feel greedy wrong       2\n",
       "3  i am ever feeling nostalgic about the fireplac...       4\n",
       "4                               i am feeling grouchy       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"one_data\",usecols=('text','target'))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc508bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ffdf75",
   "metadata": {},
   "source": [
    "# Text Preprocessesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9852a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2756b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llematization(text):\n",
    "    lm=WordNetLemmatizer()\n",
    "    text=text.split()\n",
    "    text=[lm.lemmatize(y) for y in text]\n",
    "    return ' '.join(text)\n",
    "def rm_stopwords(text):\n",
    "    english_stopword=set(stopwords.words('english')).union(set(STOP_WORDS))\n",
    "    text=[i for i in str(text).split() if i not in english_stopword ]\n",
    "    return ' '.join(text)\n",
    "def lower_case(text):\n",
    "    text=text.split()\n",
    "    text=[y.lower() for y in text ]\n",
    "    return' '.join(text)\n",
    "def select_only_char(text):\n",
    "    text=re.sub(r'[^a-z]', ' ', text)\n",
    "    text=' '.join(text.split())\n",
    "    return text.strip()\n",
    "def normalize_text(data):\n",
    "    data.text=data.text.apply(lambda text : llematization(text))\n",
    "    data.text=data.text.apply(lambda text : rm_stopwords(text))\n",
    "    data.text=data.text.apply(lambda text : lower_case(text))\n",
    "    data.text=data.text.apply(lambda text : select_only_char(text))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928d45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=normalize_text(data.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4dce40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humiliated</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feeling hopeless damned hopeful care awake</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing minute post feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feeling nostalgic fireplace know property</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110582</th>\n",
       "      <td>my friend i took trip hampton basslights booke...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110583</th>\n",
       "      <td>check departure staff friendly professional wa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110584</th>\n",
       "      <td>this hampton located quiet street hospital loc...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110585</th>\n",
       "      <td>awesome wing my favorite wa garlic parmesan gr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110586</th>\n",
       "      <td>clean facility freeway staff friendly efficien...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110565 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  target\n",
       "0                                   didnt feel humiliated       7\n",
       "1              feeling hopeless damned hopeful care awake       7\n",
       "2               im grabbing minute post feel greedy wrong       2\n",
       "3               feeling nostalgic fireplace know property       4\n",
       "4                                         feeling grouchy       2\n",
       "...                                                   ...     ...\n",
       "110582  my friend i took trip hampton basslights booke...       4\n",
       "110583  check departure staff friendly professional wa...       5\n",
       "110584  this hampton located quiet street hospital loc...       5\n",
       "110585  awesome wing my favorite wa garlic parmesan gr...       5\n",
       "110586  clean facility freeway staff friendly efficien...       4\n",
       "\n",
       "[110565 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32cb4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemer=SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a69951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9e3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef9934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7e161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432cecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb543d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a279d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc6826f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [stemer.stem(token) for token in word_tokenize(text) if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "172e506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(tokenizer=tokenizer,stop_words=set(stopwords.words('english')).union(set(STOP_WORDS)),ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8048743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kabir\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'n', 'need', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'quit', 'realli', 'regard', 'sever', 'sha', 'sinc', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'use', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<110565x781165 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3780837 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e3f4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aa bb', 'aa bit', 'aa center', 'aa era', 'aa forc',\n",
       "       'aa ginoo', 'aa good', 'aa hanahmasitah', 'aa meet', 'aa program',\n",
       "       'aa read', 'aa sent', 'aaa', 'aaa aarp', 'aaa advertis',\n",
       "       'aaa approv', 'aaa automobil', 'aaa award', 'aaa card',\n",
       "       'aaa church', 'aaa club', 'aaa diamond', 'aaa discount',\n",
       "       'aaa francoisbaroin', 'aaa good', 'aaa member', 'aaa mention',\n",
       "       'aaa mind', 'aaa night', 'aaa opportun', 'aaa packag',\n",
       "       'aaa portion', 'aaa price', 'aaa properti', 'aaa rate',\n",
       "       'aaa similar', 'aaa tour', 'aaa travel', 'aaa type', 'aaa vai',\n",
       "       'aaa wa', 'aaaa', 'aaaaaaaaaaaaa', 'aaaaaaah', 'aaaaaaah beauti',\n",
       "       'aaaaaaand', 'aaaaaarggh', 'aaaaaarggh unodey', 'aaaaahh',\n",
       "       'aaaaahh quot', 'aaaaahhhhh', 'aaaaand', 'aaaaand near',\n",
       "       'aaaaand ti', 'aaaah', 'aaaahh', 'aaaahh dog', 'aaaahhh',\n",
       "       'aaaahhh time', 'aaaand', 'aaaand drop', 'aaaand lawnmow',\n",
       "       'aaaand steal', 'aaaannnddd', 'aaaannnddd len', 'aaah',\n",
       "       'aaah proud', 'aaahh', 'aaahh total', 'aaahhh', 'aaahhh cake',\n",
       "       'aaahhh im', 'aaalllllli', 'aaalllllli left', 'aaand',\n",
       "       'aaand hour', 'aaawww', 'aaawww day', 'aac', 'aac feel', 'aadido',\n",
       "       'aadido durant', 'aafter', 'aafter thought', 'aah', 'aah echt',\n",
       "       'aaha', 'aaha c', 'aaha wa', 'aahaha', 'aahaha hilari', 'aahhh',\n",
       "       'aahhh feel', 'aahhh sound', 'aahhh work', 'aahhhhh',\n",
       "       'aahhhhh bad', 'aaishasalman', 'aaishasalman mera', 'aalliesun',\n",
       "       'aalliesun lmaoo', 'aalshibl', 'aalshibl play', 'aam', 'aam het',\n",
       "       'aamiainen', 'aamiainen hyv', 'aamiaisen', 'aamiaisen puuttuminen',\n",
       "       'aamiaisen sai', 'aamiaista', 'aamiaista varten', 'aamir',\n",
       "       'aamir khan', 'aamir whacki', 'aamuksi', 'aamuksi bostonin',\n",
       "       'aamulla', 'aamulla ja', 'aamupala', 'aamupala ja', 'aamupala oli',\n",
       "       'aamupala sek', 'aan', 'aan attack', 'aan de', 'aan decemb',\n",
       "       'aan die', 'aan een', 'aan en', 'aan geen', 'aan het',\n",
       "       'aan hobbyen', 'aan kind', 'aan mij', 'aan mijn', 'aan mn',\n",
       "       'aan monteren', 'aan n', 'aan schilderen', 'aan strak', 'aan te',\n",
       "       'aan van', 'aan vervang', 'aan werk', 'aan zeker', 'aan zitten',\n",
       "       'aan zwart', 'aanbellen', 'aanbellen en', 'aanbellen voor',\n",
       "       'aanchaul', 'aanchaul hate', 'aandacht', 'aandacht al',\n",
       "       'aandacht gaat', 'aandeslag', 'aandeslag werkaandewinkel',\n",
       "       'aanfietsen', 'aanfietsen naar', 'aangeboden',\n",
       "       'aangeboden weekendj', 'aangebroken', 'aangebroken fles',\n",
       "       'aangekle', 'aangekle nu', 'aangemaakt', 'aangemaakt surpris',\n",
       "       'aangenam', 'aangenam verrass', 'aanrad', 'aanrad voor', 'aant',\n",
       "       'aant maken', 'aant w', 'aantal', 'aantal uren', 'aanwezig',\n",
       "       'aanwezig wifi', 'aanwinst', 'aap', 'aap mere', 'aar', 'aardappel',\n",
       "       'aardappel worden', 'aardig', 'aardig van', 'aaron', 'aaron came',\n",
       "       'aaron coook', 'aaron copland', 'aaron departur', 'aaron foremost',\n",
       "       'aaron gave', 'aaron ha', 'aaron r', 'aaroncurri',\n",
       "       'aaroncurri tell', 'aaronmrichard', 'aaronmrichard want', 'aarp',\n",
       "       'aarp aaa', 'aarp aarp', 'aarp card', 'aarp convent',\n",
       "       'aarp discount', 'aarp final', 'aarp guest', 'aarp overnight',\n",
       "       'aarp rate', 'aarp room', 'aasman', 'aasman heal', 'aausbrook',\n",
       "       'aausbrook favorit', 'ab', 'ab belli', 'ab die', 'ab easi',\n",
       "       'ab everi', 'ab freak', 'ab help', 'ab homework', 'ab love',\n",
       "       'ab machin', 'ab wenn', 'ab work', 'abaaabyy', 'aback',\n",
       "       'aback look', 'aback new', 'aback priyanka', 'aback sign',\n",
       "       'aback staff', 'aback unfriend', 'aback walk', 'abadint',\n",
       "       'abadint soutient', 'abaga', 'abaixo', 'abaixo som', 'abandon',\n",
       "       'abandon ask', 'abandon bathroom', 'abandon believ',\n",
       "       'abandon build', 'abandon busi', 'abandon cant', 'abandon chat',\n",
       "       'abandon check', 'abandon construct', 'abandon dream',\n",
       "       'abandon drink', 'abandon embrac', 'abandon exercis',\n",
       "       'abandon face', 'abandon favor', 'abandon feel', 'abandon fix',\n",
       "       'abandon foolish', 'abandon ha', 'abandon halfway', 'abandon hope',\n",
       "       'abandon key', 'abandon like', 'abandon littl', 'abandon love',\n",
       "       'abandon militari', 'abandon need', 'abandon onlin',\n",
       "       'abandon plan', 'abandon plate', 'abandon poor', 'abandon project',\n",
       "       'abandon puppi', 'abandon race', 'abandon realiti', 'abandon sake',\n",
       "       'abandon speak', 'abandon staff', 'abandon teach', 'abandon trace',\n",
       "       'abandon twitter', 'abandon vacuum', 'abandon way',\n",
       "       'abandon wonder', 'abandona', 'abandona uno', 'abandonado',\n",
       "       'abandonado casa', 'abanico', 'abanico de', 'abassi',\n",
       "       'abassi phir', 'abat', 'abat lost', 'abba', 'abba wa',\n",
       "       'abbastanza', 'abbastanza buono', 'abbastanza central',\n",
       "       'abbastanza confortevol', 'abbastanza dozzinal',\n",
       "       'abbastanza pulit', 'abbastanza soddisfacent', 'abbastanza solari',\n",
       "       'abbastanza tranquillo', 'abbey', 'abbey amaz', 'abbey big',\n",
       "       'abbey book', 'abbey chang', 'abbey charg', 'abbey day',\n",
       "       'abbey differ', 'abbey everyday', 'abbey everyth', 'abbey fantast',\n",
       "       'abbey friday', 'abbey given', 'abbey got', 'abbey guest',\n",
       "       'abbey inn', 'abbey marina', 'abbey nearbi', 'abbey outdoor',\n",
       "       'abbey peopl', 'abbey perfect', 'abbey personnel', 'abbey play',\n",
       "       'abbey porto', 'abbey resort', 'abbey room', 'abbey set',\n",
       "       'abbey staff', 'abbey stay', 'abbey truli', 'abbey venu',\n",
       "       'abbey wa', 'abbey wed', 'abbi', 'abbi cowork', 'abbi home',\n",
       "       'abbi naomi', 'abbi wa', 'abbiamo', 'abbiamo anch',\n",
       "       'abbiamo apprezzato', 'abbiamo avuto', 'abbiamo caricato',\n",
       "       'abbiamo dormito', 'abbiamo pi', 'abbiamo portato',\n",
       "       'abbiamo scelto', 'abbiamo sfruttato', 'abbiamo soggiornato',\n",
       "       'abbiamo speso', 'abbiamo trascorso', 'abbiano',\n",
       "       'abbiano cambiato', 'abbiano rimesso', 'abbiesparkl',\n",
       "       'abbiesparkl best', 'abbigail', 'abbigail apraxia',\n",
       "       'abbigliamento', 'abbigliamento e', 'abbondant', 'abbondant e',\n",
       "       'abbondant la', 'abbott', 'abbott kinney', 'abbott said',\n",
       "       'abbozzando', 'abbozzando qualcosa', 'abbykincanon',\n",
       "       'abbykincanon loserlouisawhoneedstwitt', 'abbymumford',\n",
       "       'abbymumford yum', 'abbysiberianhuski', 'abbysiberianhuski thank',\n",
       "       'abbysik', 'abbysik choic', 'abc', 'abc cbs', 'abc conveni',\n",
       "       'abc cost', 'abc countrymus', 'abc famili', 'abc law', 'abc link',\n",
       "       'abc market', 'abc nbc', 'abc news', 'abc room', 'abc store',\n",
       "       'abc studio', 'abc type', 'abcfamili', 'abcfamili harrypott',\n",
       "       'abd', 'abd al', 'abd bathroom', 'abd charg', 'abd hear',\n",
       "       'abd help', 'abd nice', 'abd paid', 'abd peg', 'abd shop',\n",
       "       'abd stay', 'abd welcom', 'abdi', 'abdi rememb', 'abdomen',\n",
       "       'abdomen ach', 'abdomen pelvi', 'abdomen regular', 'abdomen seen',\n",
       "       'abdomen wa', 'abdomin', 'abdomin pain', 'abdomin surgeri',\n",
       "       'abduct', 'abduct daughter', 'abdul', 'abdul al', 'abe',\n",
       "       'abe lincoln', 'abegab', 'abegab born', 'abelard', 'abem',\n",
       "       'abem oral', 'abend', 'abend die', 'abend ein', 'abend verbracht',\n",
       "       'abendbuffet', 'abendbuffet im', 'abendessen', 'abendessen ect',\n",
       "       'abendessen ist', 'abenteu', 'abenteu e', 'aber', 'aber auch',\n",
       "       'aber ausreichend', 'aber balkon', 'aber da', 'aber der',\n",
       "       'aber die', 'aber e', 'aber einen', 'aber empfehlenswert',\n",
       "       'aber erstmal', 'aber etwa', 'aber fr', 'aber freundlich',\n",
       "       'aber gar', 'aber kein', 'aber lebendig', 'aber mit', 'aber nicht',\n",
       "       'aber noch', 'aber ordnung', 'aber professionel', 'aber ruhig',\n",
       "       'aber sehr', 'aber sonst', 'aber vergleichsweis', 'aber vorhanden',\n",
       "       'aber wir', 'aber zu', 'aber zur', 'abercrombi', 'abercrombi came',\n",
       "       'aberdeen', 'aberdeen charg', 'abernathi', 'abernathi great',\n",
       "       'aberr', 'aberr wa', 'abessey', 'abessey love', 'abet',\n",
       "       'abet quest', 'abfallprodukt', 'abfallprodukt total', 'abgebaut',\n",
       "       'abgebaut und', 'abgenommen', 'abgenommen habe', 'abgenutzt',\n",
       "       'abgeschirmt', 'abgeschirmt wa', 'abgestorben',\n",
       "       'abgestorben pflanzen', 'abgewohnt'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654ae938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781165"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7701fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6b5981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110565,) (110565, 781165)\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.shape} {inputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb1f168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3647fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 5613\n",
    "\n",
    "train_inputs = inputs[:train_size]\n",
    "train_targets = y[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "561e9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs = inputs[train_size:]\n",
    "val_targets = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93260eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4197398895421343 0.2669410778260538\n"
     ]
    }
   ],
   "source": [
    "rf= RandomForestClassifier(n_estimators=60, max_depth=3,n_jobs=-1, random_state=42)\n",
    "rf.fit(train_inputs, train_targets)\n",
    "train_accuracy = accuracy_score (train_targets, rf.predict(train_inputs))\n",
    "val_accuracy = accuracy_score(val_targets, rf.predict(val_inputs))\n",
    "print(f'{train_accuracy} {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22854915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882415820416889 0.3060446680387225\n"
     ]
    }
   ],
   "source": [
    "rf=LogisticRegression(max_iter=2500, solver='newton-cg', C=1.5, tol=1e-5)\n",
    "rf.fit(train_inputs, train_targets)\n",
    "train_accuracy = accuracy_score (train_targets, rf.predict(train_inputs))\n",
    "val_accuracy = accuracy_score(val_targets, rf.predict(val_inputs))\n",
    "print(f'{train_accuracy} {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73aca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304f7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d1fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade5888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df9d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6585579",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data['text']\n",
    "y=data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b670b",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f36d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"neural networks libraries\"\"\"\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer,BatchNormalization,Flatten,Embedding,LSTM,Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import softmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\"\"\" sklearns libraries \"\"\"\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ef2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(x,y,random_state=4,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf7f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(stop_words='english')\n",
    "X_train=vectorizer.fit_transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c56e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<88452x58978 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1460135 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc651995",
   "metadata": {},
   "outputs": [],
   "source": [
    "+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
